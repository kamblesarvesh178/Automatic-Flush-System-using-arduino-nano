{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "name": "emotion detection",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10338964,
          "sourceType": "datasetVersion",
          "datasetId": 6402105
        },
        {
          "sourceId": 10340674,
          "sourceType": "datasetVersion",
          "datasetId": 6403201
        }
      ],
      "dockerImageVersionId": 30823,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamblesarvesh178/Automatic-Flush-System-using-arduino-nano/blob/main/emotion_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Advanced Emotion Recognition from Facial Images and Speech Signals using Complex-Value Spatio-Temporal Graph Convolutional Neural Networks***"
      ],
      "metadata": {
        "id": "60cIqPJG3Ppw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive"
      ],
      "metadata": {
        "id": "PM1RUghb0JoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount drive code\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ivjWVK6m0BUb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install libraries"
      ],
      "metadata": {
        "id": "k9yQhBDKzFTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spektral"
      ],
      "metadata": {
        "id": "nJZXsXMOy32H",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:16:42.011474Z",
          "iopub.execute_input": "2024-12-31T08:16:42.01175Z",
          "iopub.status.idle": "2024-12-31T08:16:47.115309Z",
          "shell.execute_reply.started": "2024-12-31T08:16:42.011728Z",
          "shell.execute_reply": "2024-12-31T08:16:47.114481Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "2shLdZEF25ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "05JJq_Qs28J5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:16:58.916277Z",
          "iopub.execute_input": "2024-12-31T08:16:58.916656Z",
          "iopub.status.idle": "2024-12-31T08:16:58.920475Z",
          "shell.execute_reply.started": "2024-12-31T08:16:58.916632Z",
          "shell.execute_reply": "2024-12-31T08:16:58.919555Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set path directories\n"
      ],
      "metadata": {
        "id": "d3TG80pkAomA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ParentDir = \"/kaggle/input/ryerson-audio-visual-database-modified/\"\n",
        "dataset_dir = ParentDir + 'Ryerson_Audio_Visual_Database_modified'\n",
        "\n",
        "#Set Paths to save features and syncronized labels\n",
        "multimodel_feature_file ='/kaggle/working/new_multimodal_features.npy'\n",
        "labels_file = '/kaggle/working/new_labels.npy'"
      ],
      "metadata": {
        "id": "gf3YpmfcAtnY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:17:12.085391Z",
          "iopub.execute_input": "2024-12-31T08:17:12.08566Z",
          "iopub.status.idle": "2024-12-31T08:17:12.089395Z",
          "shell.execute_reply.started": "2024-12-31T08:17:12.085639Z",
          "shell.execute_reply": "2024-12-31T08:17:12.088572Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data"
      ],
      "metadata": {
        "id": "MxSZ-KAXxO5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "emotion_mapping = {\n",
        "    '01': 'neutral',\n",
        "    '02': 'calm',\n",
        "    '03': 'happy',\n",
        "    '04': 'sad',\n",
        "    '05': 'angry',\n",
        "    '06': 'fearful',\n",
        "    '07': 'disgust',\n",
        "    '08': 'surprised'\n",
        "}\n",
        "\n",
        "# Create a dictionary to store files by emotion only\n",
        "file_dict = defaultdict(list)\n",
        "\n",
        "# Parse through each actor's folder and categorize files by emotion\n",
        "for actor_folder in os.listdir(dataset_dir):\n",
        "    actor_path = os.path.join(dataset_dir, actor_folder)\n",
        "\n",
        "    # Check if it's a directory\n",
        "    if os.path.isdir(actor_path):\n",
        "        for file in os.listdir(actor_path):\n",
        "            if file.endswith('.mp4'):\n",
        "                parts = file.split('-')\n",
        "                emotion = parts[2]  # Only consider the emotion part of the filename\n",
        "                # Add file to dictionary, storing the full path\n",
        "                file_dict[emotion_mapping[str(emotion)]].append(os.path.join(actor_path, file))\n",
        "\n",
        "for key, value in file_dict.items():\n",
        "    print(f\"{key}: {len(value)}\")"
      ],
      "metadata": {
        "id": "7BoX3mAmxRF8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:17:24.315408Z",
          "iopub.execute_input": "2024-12-31T08:17:24.315676Z",
          "iopub.status.idle": "2024-12-31T08:17:24.366391Z",
          "shell.execute_reply.started": "2024-12-31T08:17:24.315656Z",
          "shell.execute_reply": "2024-12-31T08:17:24.365568Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding the shorter list with None to match the length of 80(max length among other cols)\n",
        "\n",
        "max_len = max(len(lst) for lst in file_dict.values())\n",
        "\n",
        "for key, value in file_dict.items():\n",
        "    while len(value) < max_len:  # If a list is shorter, pad it with None\n",
        "        value.append(None)\n",
        "\n",
        "# Now, create the DataFrame\n",
        "df = pd.DataFrame(file_dict)\n",
        "\n",
        "df.T\n",
        "\n",
        "df.T.to_csv('file_dict.csv')"
      ],
      "metadata": {
        "id": "m3F6NXJfyLoj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:17:29.732421Z",
          "iopub.execute_input": "2024-12-31T08:17:29.732701Z",
          "iopub.status.idle": "2024-12-31T08:17:29.745756Z",
          "shell.execute_reply.started": "2024-12-31T08:17:29.732679Z",
          "shell.execute_reply": "2024-12-31T08:17:29.744667Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_df = df.T.stack().reset_index()\n",
        "\n",
        "# Rename the columns appropriately\n",
        "flattened_df.columns = ['label','index', 'filename']\n",
        "\n",
        "# Drop the unnecessary 'index' column\n",
        "flattened_df = flattened_df.drop(columns=['index'])\n",
        "\n",
        "# Display or save the flattened DataFrame\n",
        "flattened_df"
      ],
      "metadata": {
        "id": "zMxAmeJs0zOZ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:17:40.043946Z",
          "iopub.execute_input": "2024-12-31T08:17:40.044265Z",
          "iopub.status.idle": "2024-12-31T08:17:40.072686Z",
          "shell.execute_reply.started": "2024-12-31T08:17:40.044238Z",
          "shell.execute_reply": "2024-12-31T08:17:40.07189Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define a custom color palette with 8 colors\n",
        "custom_colors = ['royalblue', 'lime', 'darkviolet', 'teal', 'red', 'gold', 'peru', 'crimson']\n",
        "\n",
        "plt.title('Count of Emotions',fontsize=12,fontweight=\"bold\")\n",
        "sns.countplot(x='label', data=flattened_df, palette=custom_colors)  # Use custom colors\n",
        "plt.ylabel('Count',fontsize=12,fontweight=\"bold\")\n",
        "plt.xlabel('Emotions',fontsize=12,fontweight=\"bold\")\n",
        "plt.xticks(fontsize=10,fontweight=\"normal\")\n",
        "plt.yticks(fontsize=10,fontweight=\"normal\")\n",
        "sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "un5qVZKw1OsS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:17:52.270099Z",
          "iopub.execute_input": "2024-12-31T08:17:52.270395Z",
          "iopub.status.idle": "2024-12-31T08:17:52.55091Z",
          "shell.execute_reply.started": "2024-12-31T08:17:52.270371Z",
          "shell.execute_reply": "2024-12-31T08:17:52.550061Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "KjaStRRTsi_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import moviepy.editor as mp\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "\n",
        "# @title Denoising - Strong Tracking Variational Bayesian Adaptive Kalman Filter\n",
        "\n",
        "class STVBAKF:\n",
        "    def __init__(self, video_path, output_audio_path, output_denoised_path):\n",
        "        self.video_path = video_path\n",
        "        self.output_audio_path = output_audio_path\n",
        "        self.output_denoised_path = output_denoised_path\n",
        "\n",
        "    # Step 1: Extract audio from the video\n",
        "    def extract_audio_from_video(self):\n",
        "        video = mp.VideoFileClip(self.video_path)\n",
        "        audio = video.audio\n",
        "        audio.write_audiofile(self.output_audio_path)\n",
        "        print(f\"Audio extracted and saved at {self.output_audio_path}\")\n",
        "\n",
        "    # Step 2: Perform noise removal on the audio\n",
        "    def remove_noise(self):\n",
        "        # Load the audio file\n",
        "        audio_signal, sr = librosa.load(self.output_audio_path, sr=None)\n",
        "        noise_removed_signal = librosa.effects.preemphasis(audio_signal)\n",
        "        return audio_signal, noise_removed_signal, sr\n",
        "\n",
        "    # Step 3: Save the denoised audio\n",
        "    def save_denoised_audio(self, denoised_signal, sr):\n",
        "        # Save the denoised audio as a .wav file\n",
        "        sf.write(self.output_denoised_path, denoised_signal, sr)\n",
        "        print(f\"Denoised audio saved at {self.output_denoised_path}\")\n",
        "\n",
        "    # Step 4: Visualize both original and denoised audio signals\n",
        "    def visualize_audio_signals(self, original_signal, denoised_signal, sr):\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Plot original audio signal\n",
        "        plt.subplot(2, 1, 1)\n",
        "        librosa.display.waveshow(original_signal, sr=sr)\n",
        "        plt.title(\"Original Audio Signal\", fontsize=12, fontweight=\"bold\")\n",
        "        plt.xlabel(\"Time (s)\", fontsize=12, fontweight=\"bold\")\n",
        "        plt.ylabel(\"Amplitude\", fontsize=12, fontweight=\"bold\")\n",
        "        plt.xticks(fontsize=12, fontweight=\"normal\")\n",
        "        plt.yticks(fontsize=12, fontweight=\"normal\")\n",
        "\n",
        "        # Plot denoised audio signal\n",
        "        plt.subplot(2, 1, 2)\n",
        "        librosa.display.waveshow(denoised_signal, sr=sr)\n",
        "        plt.title(\"Denoised Audio Signal\", fontsize=12, fontweight=\"bold\")\n",
        "        plt.xlabel(\"Time (s)\", fontsize=12, fontweight=\"bold\")\n",
        "        plt.ylabel(\"Amplitude\", fontsize=12, fontweight=\"bold\")\n",
        "        plt.xticks(fontsize=12, fontweight=\"normal\")\n",
        "        plt.yticks(fontsize=12, fontweight=\"normal\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Method to process the entire audio denoising workflow\n",
        "    def process_audio(self):\n",
        "        self.extract_audio_from_video()\n",
        "        original_signal, denoised_signal, sr = self.remove_noise()\n",
        "        self.save_denoised_audio(denoised_signal, sr)\n",
        "        self.visualize_audio_signals(original_signal, denoised_signal, sr)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "video_path = '/kaggle/input/ryerson-audio-visual-database-modified/Ryerson_Audio_Visual_Database_modified/Actor_01/01-01-01-01-01-01-01.mp4'\n",
        "output_audio_path = '/kaggle/working/extracted_audio.wav'\n",
        "output_denoised_path = '/kaggle/working/denoised_audio.wav'\n",
        "\n",
        "# Instantiate the class and process the audio\n",
        "stvbakf = STVBAKF(video_path, output_audio_path, output_denoised_path)\n",
        "stvbakf.process_audio()"
      ],
      "metadata": {
        "id": "OU87UfuyvDun",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:18:01.500936Z",
          "iopub.execute_input": "2024-12-31T08:18:01.501224Z",
          "iopub.status.idle": "2024-12-31T08:18:16.105741Z",
          "shell.execute_reply.started": "2024-12-31T08:18:01.501194Z",
          "shell.execute_reply": "2024-12-31T08:18:16.10492Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# @title Normalizing - Kernel-based Ensemble Gaussian Mixture Filtering\n",
        "\n",
        "class KEGMF:\n",
        "    def __init__(self, video_path, output_path, num_frames=5):\n",
        "        self.video_path = video_path\n",
        "        self.output_path = output_path\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "    # Method to normalize and visualize video frames\n",
        "    def visualize_and_save_normalized_video(self):\n",
        "        # Open the video file\n",
        "        cap = cv2.VideoCapture(self.video_path)\n",
        "\n",
        "        # Check if the video was opened successfully\n",
        "        if not cap.isOpened():\n",
        "            print(\"Error opening video file.\")\n",
        "            return\n",
        "\n",
        "        # Get the width, height, and frames per second (fps) of the input video\n",
        "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        # Define the codec and create VideoWriter object to save normalized video\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\n",
        "        out = cv2.VideoWriter(self.output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "        # Initialize a frame counter\n",
        "        frame_count = 0\n",
        "\n",
        "        # Loop through the video frames\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Normalize the frame by scaling pixel values to the range [0, 1]\n",
        "            frame_normalized = frame.astype(np.float32) / 255.0\n",
        "\n",
        "            # Write the normalized frame to the output video (convert it back to BGR)\n",
        "            out.write((frame_normalized * 255).astype(np.uint8))\n",
        "\n",
        "            # Only visualize the first few frames\n",
        "            if frame_count < self.num_frames:\n",
        "                # Convert BGR (OpenCV default) to RGB for Matplotlib\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame_normalized_rgb = cv2.cvtColor((frame_normalized * 255).astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Plot the original and normalized frames side by side\n",
        "                plt.figure(figsize=(10, 5))\n",
        "                plt.subplot(1, 2, 1)\n",
        "                plt.imshow(frame_rgb)\n",
        "                plt.title(f\"Original Frame {frame_count + 1}\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.subplot(1, 2, 2)\n",
        "                plt.imshow(frame_normalized_rgb)\n",
        "                plt.title(f\"Normalized Frame {frame_count + 1}\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "            # Increase frame counter\n",
        "            frame_count += 1\n",
        "\n",
        "        # Release the video capture and writer objects\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        print(f\"Normalized video saved at {self.output_path}\")\n",
        "\n",
        "# Example usage\n",
        "video_path = '/kaggle/input/ryerson-audio-visual-database-modified/Ryerson_Audio_Visual_Database_modified/Actor_01/01-01-03-01-02-02-01.mp4'\n",
        "output_path = '/kaggle/working/normalized_output.mp4'\n",
        "\n",
        "# Instantiate the class and process the video\n",
        "kegmf = KEGMF(video_path, output_path, num_frames=2)\n",
        "kegmf.visualize_and_save_normalized_video()"
      ],
      "metadata": {
        "id": "wm-DH58Mt1d8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:19:30.40459Z",
          "iopub.execute_input": "2024-12-31T08:19:30.405109Z",
          "iopub.status.idle": "2024-12-31T08:19:32.343518Z",
          "shell.execute_reply.started": "2024-12-31T08:19:30.405081Z",
          "shell.execute_reply": "2024-12-31T08:19:32.342713Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extraction"
      ],
      "metadata": {
        "id": "DMzVCxKHIAnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import moviepy.editor as mp\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from torchvision.models import vit_b_16\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# @title Feature selection - Bayesian Weighted Random Forest (BWRF)\n",
        "\n",
        "# Step 1: Extract audio directly from the MP4 file in memory using MoviePy\n",
        "def extract_audio_from_video_in_memory(mp4_path):\n",
        "    video = mp.VideoFileClip(mp4_path)\n",
        "    audio_path = \"/tmp/temp_audio.wav\"  # Temporary path for audio extraction\n",
        "\n",
        "    # Extract audio from video and write it to a temporary file\n",
        "    video.audio.write_audiofile(audio_path, logger=None)\n",
        "\n",
        "    # Load audio using Librosa\n",
        "    y, sr = librosa.load(audio_path, sr=None)  # Load at original sampling rate\n",
        "    return y, sr, video  # Return audio and video together\n",
        "\n",
        "# Step 3: BWRF for Feature Selection\n",
        "def bwrf_feature_selection(mfcc_features, labels, n_trees=100):\n",
        "    \"\"\"\n",
        "    Bayesian Weighted Random Forest for feature selection of MFCC features.\n",
        "\n",
        "    \"\"\"\n",
        "    # Step 1: Train a Random Forest model\n",
        "    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    rf.fit(mfcc_features, labels)\n",
        "\n",
        "    # Step 2: Collect feature importances from each tree\n",
        "    tree_importances = np.array([tree.feature_importances_ for tree in rf.estimators_])\n",
        "\n",
        "    # Step 3: Calculate performance-based weights (tree accuracy)\n",
        "    tree_weights = np.array([calculate_tree_weight(tree, mfcc_features, labels) for tree in rf.estimators_])\n",
        "\n",
        "    # Step 4: Normalize the weights using Bayesian update (Softmax function)\n",
        "    tree_weights = np.exp(tree_weights) / np.sum(np.exp(tree_weights))\n",
        "\n",
        "    # Step 5: Weighted feature importances\n",
        "    weighted_feature_importances = np.average(tree_importances, axis=0, weights=tree_weights)\n",
        "\n",
        "    # Step 6: Sort features by importance\n",
        "    sorted_indices = np.argsort(weighted_feature_importances)[::-1]\n",
        "    sorted_feature_importances = weighted_feature_importances[sorted_indices]\n",
        "\n",
        "    return sorted_indices, sorted_feature_importances\n",
        "\n",
        "def calculate_tree_weight(tree, X, y):\n",
        "    tree_preds = tree.predict(X)\n",
        "    accuracy = np.mean(tree_preds == y)\n",
        "    return accuracy\n",
        "\n",
        "# Step 3: MFCC extraction using torch (GPU enabled)\n",
        "def extract_mfcc_features_torch(audio_array, sr, frame_length=2048, hop_length=512, n_mfcc=13, n_mels=40, n_fft=2048):\n",
        "    # Move audio array to GPU\n",
        "    audio_tensor = torch.tensor(audio_array, device=device)\n",
        "\n",
        "    # Compute Short-Time Fourier Transform (STFT)\n",
        "    stft = torch.stft(audio_tensor, n_fft=n_fft, hop_length=hop_length, win_length=frame_length, return_complex=True)\n",
        "\n",
        "    # Calculate the power spectrogram\n",
        "    spectrogram = torch.abs(stft) ** 2\n",
        "\n",
        "    # Mel filter banks on GPU\n",
        "    mel_filters = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels)\n",
        "    mel_filters = torch.tensor(mel_filters, device=device)\n",
        "\n",
        "    # Apply Mel filter banks to the power spectrogram\n",
        "    mel_spectrogram = torch.matmul(mel_filters, spectrogram)\n",
        "\n",
        "    # Take the log of the Mel spectrogram\n",
        "    log_mel_spectrogram = torch.log(mel_spectrogram + 1e-10)\n",
        "\n",
        "    # Compute DCT (Discrete Cosine Transform) to get MFCCs (using librosa for DCT)\n",
        "    mfcc = torch.tensor(librosa.feature.mfcc(sr=sr, S=log_mel_spectrogram.cpu().numpy(), n_mfcc=n_mfcc), device=device)\n",
        "\n",
        "    return mfcc.T  # Return transposed MFCCs for time frames as rows"
      ],
      "metadata": {
        "id": "hUsCSK7ZUZTG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:20:07.90818Z",
          "iopub.execute_input": "2024-12-31T08:20:07.908488Z",
          "iopub.status.idle": "2024-12-31T08:20:12.648936Z",
          "shell.execute_reply.started": "2024-12-31T08:20:07.908465Z",
          "shell.execute_reply": "2024-12-31T08:20:12.648227Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feature extraction - Dual Vision Transformer (DVT)\n",
        "\n",
        "# Step 3: Preprocess video frames for CNN input (adapted for Vision Transformer)\n",
        "def preprocess_frame(frame):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((224, 224)),  # Resize frame for ViT\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    return transform(frame)\n",
        "\n",
        "# Step 4: Extract video frames and ViT-based features using GPU\n",
        "# Function to extract video frames and ViT-based features using GPU\n",
        "def extract_video_frames_and_features(video, skip_frames=1):\n",
        "    # Load two Vision Transformer models (ViT)\n",
        "    vit_model_1 = vit_b_16(pretrained=True).to(device)  # First Vision Transformer\n",
        "    vit_model_2 = vit_b_16(pretrained=True).to(device)  # Second Vision Transformer\n",
        "\n",
        "    # Set both models to evaluation mode\n",
        "    vit_model_1.eval()\n",
        "    vit_model_2.eval()\n",
        "\n",
        "    video_fps = video.fps\n",
        "    total_frames = int(video.duration * video_fps)  # Total number of frames in the video\n",
        "\n",
        "    cnn_features = []\n",
        "\n",
        "    for i in range(0, total_frames, skip_frames):  # Step through video frames with the defined skip step\n",
        "        frame = video.get_frame(i / video_fps)  # Get frame at specific timestamp\n",
        "        frame_tensor = preprocess_frame(frame).unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
        "\n",
        "        # Extract features from both ViT models\n",
        "        with torch.no_grad():\n",
        "            # Pass the frame through both Vision Transformers\n",
        "            features_vit_1 = vit_model_1(frame_tensor).squeeze().cpu().numpy()  # Move to CPU\n",
        "            features_vit_2 = vit_model_2(frame_tensor).squeeze().cpu().numpy()  # Move to CPU\n",
        "\n",
        "            # Concatenate features from both models\n",
        "            combined_features = np.concatenate((features_vit_1, features_vit_2), axis=0)\n",
        "            cnn_features.append(combined_features)\n",
        "\n",
        "    return np.array(cnn_features)"
      ],
      "metadata": {
        "id": "3B3D8_7vsN7x",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:20:28.438869Z",
          "iopub.execute_input": "2024-12-31T08:20:28.439364Z",
          "iopub.status.idle": "2024-12-31T08:20:28.446115Z",
          "shell.execute_reply.started": "2024-12-31T08:20:28.439339Z",
          "shell.execute_reply": "2024-12-31T08:20:28.445198Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feature fusion and syncronization - Multi-Tensor Fusion Network\n",
        "\n",
        "# Step 5 : Feature fusion and Synchronize MFCC and video features\n",
        "def multi_tensor_fusion(mfcc_features, sr, hop_length, video_fps, video_cnn_features, skip_frames=1):\n",
        "    mfcc_frame_duration = hop_length / sr  # Duration of each MFCC frame in seconds\n",
        "    video_frame_duration = 1 / video_fps  # Duration of each video frame in seconds\n",
        "    multimodal_features = []\n",
        "\n",
        "    for i in range(0, len(video_cnn_features)):  # Only loop through extracted video frames\n",
        "        # Calculate the timestamp of the skipped video frame\n",
        "        video_timestamp = i * skip_frames * video_frame_duration\n",
        "\n",
        "        # Find the corresponding MFCC frame index based on the video frame's timestamp\n",
        "        mfcc_index = int(video_timestamp / mfcc_frame_duration)\n",
        "\n",
        "        # Ensure the MFCC index is within the range of available MFCC frames\n",
        "        if mfcc_index < mfcc_features.shape[0]:\n",
        "            mfcc_for_frame = mfcc_features[mfcc_index].cpu().numpy()  # Move MFCC features to CPU and convert to NumPy\n",
        "        else:\n",
        "            mfcc_for_frame = np.zeros(mfcc_features.shape[1])  # Zero padding if MFCC frame is missing\n",
        "\n",
        "        # Get CNN-based video features and move them to CPU before converting to NumPy\n",
        "        video_cnn_features_cpu = video_cnn_features[i]\n",
        "\n",
        "        # Feature fusion of  MFCC features and CNN features for this frame\n",
        "        combined_features = np.concatenate((mfcc_for_frame, video_cnn_features_cpu), axis=0)\n",
        "        multimodal_features.append(combined_features)\n",
        "\n",
        "    return np.array(multimodal_features)\n",
        "\n",
        "\n",
        "# Step 6: Full pipeline for extracting multimodal features\n",
        "def audio_video_emotion_recognition(mp4_path, label=None, skip_frames=5):\n",
        "    # Extract audio and video from the video file\n",
        "    audio_array, sr, video = extract_audio_from_video_in_memory(mp4_path)\n",
        "\n",
        "    # Extract MFCC features from audio using torch\n",
        "    mfcc_features = extract_mfcc_features_torch(audio_array, sr)\n",
        "\n",
        "\n",
        "    # Extract CNN-based features from video frames (with frame skipping)\n",
        "    video_cnn_features = extract_video_frames_and_features(video, skip_frames=skip_frames)\n",
        "\n",
        "\n",
        "    # Synchronize and combine MFCC and CNN-based video features\n",
        "    multimodal_features = multi_tensor_fusion(mfcc_features, sr, hop_length=512, video_fps=video.fps, video_cnn_features=video_cnn_features, skip_frames=skip_frames)\n",
        "\n",
        "\n",
        "    return multimodal_features, label"
      ],
      "metadata": {
        "id": "bJVMrJ72si4J",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:20:48.185461Z",
          "iopub.execute_input": "2024-12-31T08:20:48.185746Z",
          "iopub.status.idle": "2024-12-31T08:20:48.192306Z",
          "shell.execute_reply.started": "2024-12-31T08:20:48.185725Z",
          "shell.execute_reply": "2024-12-31T08:20:48.191466Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 7: Prepare dataset\n",
        "mp4_paths = flattened_df['filename']  # List of MP4 file paths\n",
        "labels = flattened_df['label']  # Corresponding labels for the videos\n",
        "\n",
        "# @title Extracting multimodal features (optional - if run once and save the features to drive)\n",
        "X = []\n",
        "y = []\n",
        "for mp4_path, label in tqdm(zip(mp4_paths, labels), total=len(mp4_paths), desc=\"Processing Videos\"):\n",
        "    features, emotion_label = audio_video_emotion_recognition(mp4_path, label, skip_frames=5)  # Skip every 5th frame\n",
        "\n",
        "    # Stack features vertically using vstack\n",
        "    if len(X) == 0:\n",
        "        X = features  # Initialize X with the first feature set\n",
        "    else:\n",
        "        X = np.vstack((X, features))  # Stack features vertically\n",
        "\n",
        "    # Append the corresponding label\n",
        "    y.extend([emotion_label] * len(features))\n",
        "\n",
        "\n",
        "# Save multimodal features and labels to files (you can save them as numpy arrays)\n",
        "np.save(multimodel_feature_file, X)\n",
        "np.save(labels_file , y)\n",
        "\n",
        "print(f\"Multimodal features saved to '{multimodel_feature_file}\")\n",
        "print(f\"Labels saved to '{labels_file}'\")"
      ],
      "metadata": {
        "id": "aqyB1XP1rlaP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:21:05.732246Z",
          "iopub.execute_input": "2024-12-31T08:21:05.732526Z",
          "iopub.status.idle": "2024-12-31T08:29:57.304428Z",
          "shell.execute_reply.started": "2024-12-31T08:21:05.732504Z",
          "shell.execute_reply": "2024-12-31T08:29:57.303549Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title load features (optional - if you load directly from the drive and when you neglect the previous code)\n",
        "\n",
        "# Load the multimodal features and labels from the saved files\n",
        "X = np.load(multimodel_feature_file)\n",
        "y = np.load(labels_file)\n",
        "\n",
        "# Verify the shapes of the loaded arrays (optional)\n",
        "print(\"Features shape:\", X.shape)\n",
        "print(\"Labels shape:\", y.shape)\n"
      ],
      "metadata": {
        "id": "esU8H0tJkEMc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:31:16.853129Z",
          "iopub.execute_input": "2024-12-31T08:31:16.853667Z",
          "iopub.status.idle": "2024-12-31T08:31:16.866056Z",
          "shell.execute_reply.started": "2024-12-31T08:31:16.853638Z",
          "shell.execute_reply": "2024-12-31T08:31:16.865338Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Model"
      ],
      "metadata": {
        "id": "2CwzRPvotxpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers, activations, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras import mixed_precision\n",
        "import tensorflow as tf\n",
        "\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# @title Complex-Value Spatio-Temporal Graph Convolutional Neural Network\n",
        "\n",
        "class ComplexReLU(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        real = tf.nn.relu(tf.math.real(inputs))\n",
        "        imag = tf.nn.relu(tf.math.imag(inputs))\n",
        "        return tf.complex(real, imag)\n",
        "\n",
        "class ComplexGraphConv(layers.Layer):\n",
        "    def __init__(self, channels, activation=None):\n",
        "        super(ComplexGraphConv, self).__init__()\n",
        "        self.activation = activation if activation else tf.identity\n",
        "\n",
        "    def call(self, real_inputs, imag_inputs, adjacency):\n",
        "        real_output = self.real_conv(real_inputs, adjacency) - self.imag_conv(imag_inputs, adjacency)\n",
        "        imag_output = self.imag_conv(real_inputs, adjacency) + self.real_conv(imag_inputs, adjacency)\n",
        "        output = tf.complex(real_output, imag_output)\n",
        "        return self.activation(output)\n",
        "\n",
        "class ComplexTemporalConv(layers.Layer):\n",
        "    def __init__(self, channels, kernel_size, activation=None):\n",
        "        super(ComplexTemporalConv, self).__init__()\n",
        "        self.real_conv = layers.Conv1D(channels, kernel_size, padding='same')\n",
        "        self.imag_conv = layers.Conv1D(channels, kernel_size, padding='same')\n",
        "        self.activation = activation if activation else tf.identity\n",
        "\n",
        "    def call(self, real_inputs, imag_inputs):\n",
        "        real_output = self.real_conv(real_inputs) - self.imag_conv(imag_inputs)\n",
        "        imag_output = self.imag_conv(real_inputs) + self.real_conv(imag_inputs)\n",
        "        output = tf.complex(real_output, imag_output)\n",
        "        return self.activation(output)\n",
        "\n",
        "class CVSTGCN_model(ComplexTemporalConv):\n",
        "    def __init__(self, input_shape, num_classes=8, dropout_rate=0.3,optimizer = None):\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=self.input_shape))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(Dropout(self.dropout_rate))\n",
        "        model.add(Dense(self.num_classes, activation='softmax', dtype='float32'))\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model"
      ],
      "metadata": {
        "id": "8S00TeOHxWXQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:31:34.24048Z",
          "iopub.execute_input": "2024-12-31T08:31:34.240767Z",
          "iopub.status.idle": "2024-12-31T08:31:42.386634Z",
          "shell.execute_reply.started": "2024-12-31T08:31:34.240742Z",
          "shell.execute_reply": "2024-12-31T08:31:42.385957Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Red-billed Blue Magpie Optimization Algorithm class\n",
        "class RedBilledBlueMagpieOptimizer:\n",
        "    def __init__(self, num_particles, num_iterations, dim, lb, ub, fitness_func):\n",
        "        self.num_particles = num_particles\n",
        "        self.num_iterations = num_iterations\n",
        "        self.dim = dim\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        self.fitness_func = fitness_func\n",
        "\n",
        "        # Initialize particles and velocities\n",
        "        self.particles = np.random.uniform(lb, ub, (num_particles, dim))\n",
        "        self.velocities = np.random.uniform(-1, 1, (num_particles, dim))\n",
        "\n",
        "        # Initialize personal best positions and fitnesses\n",
        "        self.personal_best_positions = np.copy(self.particles)\n",
        "        self.personal_best_fitness = np.full(num_particles, float('inf'))\n",
        "\n",
        "        # Global best position and fitness\n",
        "        self.global_best_position = np.zeros(dim)\n",
        "        self.global_best_fitness = float('inf')\n",
        "\n",
        "        # To store fitness history for plotting\n",
        "        self.fitness_history = []\n",
        "\n",
        "    def optimize(self):\n",
        "        for iteration in range(self.num_iterations):\n",
        "            for i in range(self.num_particles):\n",
        "                # Calculate the fitness of each particle\n",
        "                fitness = self.fitness_func(self.particles[i])\n",
        "\n",
        "                # Update personal best position and fitness if better\n",
        "                if fitness < self.personal_best_fitness[i]:\n",
        "                    self.personal_best_fitness[i] = fitness\n",
        "                    self.personal_best_positions[i] = self.particles[i]\n",
        "\n",
        "                # Update global best if the current particle is better\n",
        "                if fitness < self.global_best_fitness:\n",
        "                    self.global_best_fitness = fitness\n",
        "                    self.global_best_position = self.particles[i]\n",
        "\n",
        "            # Update particles' velocities and positions\n",
        "            for i in range(self.num_particles):\n",
        "                r1 = np.random.rand(self.dim)\n",
        "                r2 = np.random.rand(self.dim)\n",
        "\n",
        "                self.velocities[i] = self.velocities[i] + r1 * (self.personal_best_positions[i] - self.particles[i]) + \\\n",
        "                                     r2 * (self.global_best_position - self.particles[i])\n",
        "\n",
        "                # Update position and keep it within bounds\n",
        "                self.particles[i] = self.particles[i] + self.velocities[i]\n",
        "                self.particles[i] = np.clip(self.particles[i], self.lb, self.ub)\n",
        "\n",
        "            # Record the global best fitness in the current iteration\n",
        "            self.fitness_history.append(self.global_best_fitness)\n",
        "\n",
        "            # Optionally print progress\n",
        "            print(f\"Iteration {iteration + 1}/{self.num_iterations}, Best Fitness: {self.global_best_fitness}\")\n",
        "\n",
        "        # Return the best fitness value at the end\n",
        "        return self.global_best_fitness\n",
        "\n",
        "\n",
        "# Define a dummy fitness function (sphere function)\n",
        "def fitness_function(x):\n",
        "    return np.sum(x ** 2)\n",
        "\n",
        "# Parameters for the optimizer\n",
        "num_particles = 30\n",
        "num_iterations = 50\n",
        "dim = 5\n",
        "lb = -10\n",
        "ub = 10\n",
        "\n",
        "# Create an instance of the optimizer\n",
        "rbmo_optimizer = RedBilledBlueMagpieOptimizer(num_particles, num_iterations, dim, lb, ub, fitness_function)\n",
        "\n",
        "# Run the optimization and get the best fitness score\n",
        "best_fitness_score = rbmo_optimizer.optimize()\n",
        "\n",
        "# Print the best fitness score at the end\n",
        "print(f\"\\nBest fitness : {best_fitness_score}\")\n"
      ],
      "metadata": {
        "id": "iaqbvRDj-7XH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:31:50.000266Z",
          "iopub.execute_input": "2024-12-31T08:31:50.00091Z",
          "iopub.status.idle": "2024-12-31T08:31:50.053076Z",
          "shell.execute_reply.started": "2024-12-31T08:31:50.000875Z",
          "shell.execute_reply": "2024-12-31T08:31:50.052262Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Step 1: Use LabelEncoder to convert string labels to integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Check the unique values in y_encoded\n",
        "print(f\"Classes : {np.unique(y)}\")\n",
        "\n",
        "# Determine the number of classes based on the unique values\n",
        "num_classes = len(np.unique(y_encoded))\n",
        "\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Step 2: One-hot encode the integer labels\n",
        "y_categorical = to_categorical(y_encoded, num_classes=num_classes)\n",
        "\n",
        "# Reshape X for CNN (adding a third dimension, i.e., X.shape[1], 1)\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting arrays\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "# Build and train the CNN model\n",
        "input_shape = (X_train.shape[1], 1)  # Input shape for Conv1D should be (timesteps, features)\n"
      ],
      "metadata": {
        "id": "49DKesaFc8u8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:32:03.405539Z",
          "iopub.execute_input": "2024-12-31T08:32:03.405883Z",
          "iopub.status.idle": "2024-12-31T08:32:03.430525Z",
          "shell.execute_reply.started": "2024-12-31T08:32:03.405851Z",
          "shell.execute_reply": "2024-12-31T08:32:03.429558Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# @title Train model\n",
        "cvstgcn_model = CVSTGCN_model(input_shape,optimizer = rbmo_optimizer.optimize())\n",
        "model = cvstgcn_model.model\n",
        "\n",
        "\n",
        "# Learning rate scheduler to reduce learning rate if validation loss doesn't improve\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
        "\n",
        "# Early stopping to stop training if the validation accuracy doesn't improve\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model with validation data and callbacks\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test),\n",
        "                    callbacks=[reduce_lr, early_stopping])\n"
      ],
      "metadata": {
        "id": "UUyQCkq1E2-B",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:32:19.565779Z",
          "iopub.execute_input": "2024-12-31T08:32:19.566122Z",
          "iopub.status.idle": "2024-12-31T08:32:50.886371Z",
          "shell.execute_reply.started": "2024-12-31T08:32:19.566096Z",
          "shell.execute_reply": "2024-12-31T08:32:50.885653Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
        "plt.title('Model Accuracy', fontsize=18, fontweight='bold')\n",
        "plt.xlabel('Epochs', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('Accuracy', fontsize=16, fontweight='bold')\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.legend(loc='lower right', fontsize=14)\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
        "plt.title('Model Loss', fontsize=18, fontweight='bold')\n",
        "plt.xlabel('Epochs', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('Loss', fontsize=16, fontweight='bold')\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "# Display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mbNzrHjOqbTO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:33:40.798509Z",
          "iopub.execute_input": "2024-12-31T08:33:40.798816Z",
          "iopub.status.idle": "2024-12-31T08:33:41.232742Z",
          "shell.execute_reply.started": "2024-12-31T08:33:40.79879Z",
          "shell.execute_reply": "2024-12-31T08:33:41.231794Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluate  model\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {score[1] * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "JGQ4cx3hE_Y5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:33:50.217174Z",
          "iopub.execute_input": "2024-12-31T08:33:50.217474Z",
          "iopub.status.idle": "2024-12-31T08:33:52.308605Z",
          "shell.execute_reply.started": "2024-12-31T08:33:50.217449Z",
          "shell.execute_reply": "2024-12-31T08:33:52.307679Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  save model\n",
        "model.save('/kaggle/working/new_model.keras')"
      ],
      "metadata": {
        "id": "KA6t-l1LPRqY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:34:27.608631Z",
          "iopub.execute_input": "2024-12-31T08:34:27.608977Z",
          "iopub.status.idle": "2024-12-31T08:34:30.633774Z",
          "shell.execute_reply.started": "2024-12-31T08:34:27.608945Z",
          "shell.execute_reply": "2024-12-31T08:34:30.632805Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "t8vO4lQuuN_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 8: Predict class labels on the test set\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class labels\n",
        "\n",
        "# Step 9: Convert one-hot encoded y_test back to class labels\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Step 10: Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Step 11: Print classification report\n",
        "class_report = classification_report(y_true, y_pred, target_names=label_encoder.classes_)\n",
        "print(\"Classification Report:\\n\", class_report)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "\n",
        "# Set title and labels with increased font size and bold\n",
        "plt.title('Confusion Matrix', fontsize=18, fontweight='bold')\n",
        "plt.ylabel('True Labels', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Predicted Labels', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Rotate x-ticks by 90 degrees and adjust tick label font size\n",
        "plt.xticks(rotation=90, fontsize=14)\n",
        "plt.yticks(rotation=0,fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "_1Up3vV3vSpA",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:34:50.879075Z",
          "iopub.execute_input": "2024-12-31T08:34:50.879403Z",
          "iopub.status.idle": "2024-12-31T08:34:51.879618Z",
          "shell.execute_reply.started": "2024-12-31T08:34:50.879376Z",
          "shell.execute_reply": "2024-12-31T08:34:51.87873Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions"
      ],
      "metadata": {
        "id": "fj6nRWv-uQW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from IPython.display import Video, display\n",
        "\n",
        "def predict_emotion(video_path, model, label_encoder):\n",
        "    \"\"\"\n",
        "    Predicts the emotion label for a given video file using the provided model and LabelEncoder.\n",
        "    Also, plays the video in Colab.\n",
        "\n",
        "    Args:\n",
        "    video_path (str): Path to the video file for which emotion is to be predicted.\n",
        "    model: Trained model for emotion prediction.\n",
        "    label_encoder: LabelEncoder used to encode the original class labels.\n",
        "\n",
        "    Returns:\n",
        "    str: Predicted class label.\n",
        "    \"\"\"\n",
        "    print(\"\\nDisplay video : \\n\")\n",
        "    # Display the video in Colab\n",
        "    display(Video(video_path, width = 320 ,embed=True))\n",
        "\n",
        "    # Extract features from the video file\n",
        "    feature_vect, _ = audio_video_emotion_recognition(video_path)\n",
        "\n",
        "    # Make a prediction using the model\n",
        "    prediction = model.predict([feature_vect])\n",
        "\n",
        "    # Get the predicted class with the highest probability\n",
        "    pred_prob = np.argmax(prediction, axis=1)\n",
        "\n",
        "    # Get the index of the most frequent predicted class\n",
        "    predicted_class_index = np.argmax(np.bincount(pred_prob))\n",
        "\n",
        "    # Decode the index back to the original label\n",
        "    predicted_class_label = label_encoder.inverse_transform([predicted_class_index])\n",
        "\n",
        "\n",
        "    return predicted_class_label[0]\n",
        "\n",
        "load_model = keras.models.load_model('/kaggle/working/new_model.keras')\n",
        "# Example usage\n",
        "video_path = \"/kaggle/input/test-data/test/01-01-05-01-01-02-08.mp4\"\n",
        "predicted_emotion = predict_emotion(video_path, load_model, label_encoder)\n",
        "print(f\"Predicted emotion: {predicted_emotion}\")\n"
      ],
      "metadata": {
        "id": "3zHfp_nrv2pa",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:40:48.343499Z",
          "iopub.execute_input": "2024-12-31T08:40:48.343879Z",
          "iopub.status.idle": "2024-12-31T08:40:54.538153Z",
          "shell.execute_reply.started": "2024-12-31T08:40:48.343847Z",
          "shell.execute_reply": "2024-12-31T08:40:54.537184Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "load_model = keras.models.load_model('/kaggle/working/new_model.keras')\n",
        "# Example usage\n",
        "video_path = \"/kaggle/input/test-data/test/01-01-06-01-01-02-09.mp4\"\n",
        "predicted_emotion = predict_emotion(video_path, load_model, label_encoder)\n",
        "print(f\"Predicted emotion: {predicted_emotion}\")"
      ],
      "metadata": {
        "id": "d8B4feOn-M1n",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:41:41.84093Z",
          "iopub.execute_input": "2024-12-31T08:41:41.841264Z",
          "iopub.status.idle": "2024-12-31T08:41:47.878722Z",
          "shell.execute_reply.started": "2024-12-31T08:41:41.841233Z",
          "shell.execute_reply": "2024-12-31T08:41:47.877867Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance metrics"
      ],
      "metadata": {
        "id": "eC9l4z0_vZnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Compute Accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Compute Precision, Recall, F1-score\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# Compute Error Rate (1 - accuracy)\n",
        "error_rate = 1 - accuracy\n",
        "\n",
        "# Specificity calculation\n",
        "tn = conf_matrix.sum() - conf_matrix.sum(axis=0) - conf_matrix.sum(axis=1) + np.diag(conf_matrix)\n",
        "fp = conf_matrix.sum(axis=0) - np.diag(conf_matrix)\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# Displaying the results\n",
        "print(\"Performance Metrics:\")\n",
        "print(\"=\"*40)\n",
        "print(f\"{'Metric':<15} {'Score':<10}\")\n",
        "print(\"=\"*40)\n",
        "print(f\"{'Accuracy':<15} : {accuracy:.4f}\")\n",
        "print(f\"{'Precision':<15} : {precision:.4f}\")\n",
        "print(f\"{'Recall':<15} : {recall:.4f}\")\n",
        "print(f\"{'F1 Score':<15} : {f1:.4f}\")\n",
        "print(f\"{'Specificity':<15} : {np.mean(specificity):.4f}\")\n",
        "print(f\"{'Error Rate':<15} : {error_rate:.4f}\")\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "id": "1jQVNVFEu_9j",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-31T08:36:42.837708Z",
          "iopub.execute_input": "2024-12-31T08:36:42.838069Z",
          "iopub.status.idle": "2024-12-31T08:36:42.852544Z",
          "shell.execute_reply.started": "2024-12-31T08:36:42.83804Z",
          "shell.execute_reply": "2024-12-31T08:36:42.851705Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}